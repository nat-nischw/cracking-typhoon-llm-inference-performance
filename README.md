# cracking-typhoon-llm-inference-performance

Cracking Typhoon LLM Inference Performance

# Installation
```
bash run.sh
```

# Demo 
```bash
python  demo_gradio.py

# with vllm
python demo_gradio_vllm.py
```

# Limitations and Discussion
The small size model (8B) in the current version 1.5X has this limitation. Although the thinking is correct, the output numbers are hallucinatory sometimes.
